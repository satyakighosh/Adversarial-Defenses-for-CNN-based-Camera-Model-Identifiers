from cgi import test
import numpy as np
import os
import random
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, AveragePooling2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.callbacks import Callback,ModelCheckpoint,LearningRateScheduler
from keras.backend.tensorflow_backend import set_session
import pickle
import math
from keras.preprocessing.image import ImageDataGenerator


#DYNAMICALLY GROW THE GPU MEMORY
print(tf.test.is_gpu_available(cuda_only=True))
config = tf.compat.v1.ConfigProto()
#config.gpu_options.per_process_gpu_memory_fraction = 0.6  # 0.6 sometimes works better for folks
config.gpu_options.allow_growth = True      
sess =  tf.compat.v1.InteractiveSession(config=config)
set_session(sess)
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'


channels = 3
IMG_SIZE = 299
DATADIR = "/home/mtech/2020/satyaki_ghosh/datasets/VISION"
CATEGORIES = [
    "D01_Samsung_GalaxyS3Mini",
    "D02_Apple_iPhone4s",
    "D03_Huawei_P9",
    "D04_LG_D290",
    "D05_Apple_iPhone5c"]
patches_per_photo = 100
img_per_model = 150
num_classes = 5


# # LOADING THE DATSET
# with open('xceptionNet/training_data.npy','rb') as f:
#     dummy = np.load(f, allow_pickle=True)
# dataset = list(dummy)


# # TRAIN-TEST SPLIT AND SHUFFLE THE PATCHES : (train,val,test) = (0.8,0.1,0.1)
# random.shuffle(dataset)
# split_ratio = 0.7
# split_num = int(len(dataset) * split_ratio)
# print(type(split_num))
# print(split_num)
# training_data = dataset[0:split_num]
# testing_data = dataset[split_num:]


# # CREATING INPUT AND LABELS
# X = []
# Y = []
# for features,label in training_data :
#   X.append(features)
#   Y.append(label)


# # PREPROCESSING
# X = np.array(X).reshape(-1,IMG_SIZE,IMG_SIZE,channels)
# X = X/255.0
# Y = np.array(Y)



# TO DECAY THE LEARNING RATE WITH INCREASING EPOCHS
def step_decay(epoch):
  initial_lrate = 0.001
  drop = 0.5
  epochs_drop = 5.0
  lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
  return lrate

lrate = LearningRateScheduler(step_decay)

# INITIALIZING FILTERS TO APPLY ON PATCHES
num_filter=3
w = np.random.rand(5,5,channels,num_filter) #changing the number of filters change accordingly the last number 
wgt = w
bias = np.zeros(num_filter)


batch_size = 32
epochs = 80

# Include the epoch in the file name (uses `str.format`)
checkpoint_path = "xceptionNet/training_1/cp_{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
checkpoint_epoch_freq = 1

# Create a callback that saves the model's weights every 5 epochs
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path, 
    verbose=1, 
    save_weights_only=True,
    save_freq='epoch' # checkpoint_epoch_freq*batch_size
  )

# Create a new model instance
import sys 
import os


image_dim = (IMG_SIZE, IMG_SIZE, 3)


# create generator
datagen = ImageDataGenerator(rescale = 1./255)
# prepare an iterators for each dataset
train_it = datagen.flow_from_directory('./xceptionNet/data/train/', class_mode='categorical', batch_size=32, target_size=(IMG_SIZE, IMG_SIZE))
val_it = datagen.flow_from_directory('./xceptionNet/data/val/', class_mode='categorical', batch_size=32, target_size=(IMG_SIZE, IMG_SIZE))
test_it = datagen.flow_from_directory('./xceptionNet/data/test/', class_mode='categorical', batch_size=32, target_size=(IMG_SIZE, IMG_SIZE))
# confirm the iterator works
batchX, batchy = train_it.next()
print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))
print(batchy.shape)




model = tf.keras.applications.xception.Xception(
    include_top=True,
    weights=None,
    input_tensor=None,
    input_shape=image_dim,
    pooling=None,
    classes=5
    #classifier_activation='softmax'
)




opt = tf.keras.optimizers.SGD(learning_rate = 0.001, momentum = 0.9, decay = 0.0005)
model.compile(loss = "categorical_crossentropy", optimizer = opt , metrics = [tf.keras.metrics.CategoricalAccuracy()])                
model.summary()


# Save the weights using the `checkpoint_path` format
model.save_weights(checkpoint_path.format(epoch=0))

# Train the model with the TWO callbacks
history=model.fit_generator(train_it, 
          epochs=epochs,
          steps_per_epoch = (num_classes * img_per_model * patches_per_photo)/batch_size,          
          callbacks=[cp_callback, lrate],
          validation_data=val_it,
          validation_steps = 8,
          verbose=1)
      
# print(history.history)
# with open('train_history_dict','wb') as f:
#     pickle.dump(history.history,f)
np.save('xceptionNet/train_history_dict_patchsize299.npy',history.history)

# LOAD THE HISTORY AS A DICT
# history=np.load('train_history_dict.npy',allow_pickle='TRUE').item()


#----------------------------------------------------------------------------------------------------------------------------------------------


# # TESTING
# # CREATING INPUT AND LABELS
# X_test = []
# Y_test = []
# for features,label in testing_data :
#   X_test.append(features)
#   Y_test.append(label)

# # PREPROCESSING
# X_test = np.array(X_test).reshape(-1,IMG_SIZE,IMG_SIZE,channels)
# X_test = X_test/255.0
# Y_test = np.array(Y_test)
# print(f'Test Data shape: {np.shape(X_test)}')

# # print(f'Checkpoint models: {os.listdir(checkpoint_dir)}')
# latest = tf.train.latest_checkpoint(checkpoint_dir)
# print(f'Latest model : {latest}')

# # Create a new model instance
# model = create_model()

# # Load the previously saved weights
# model.load_weights(latest)

# Re-evaluate the model
# loss, acc = model.evaluate(X_test, Y_test, verbose=2)
# yhat = model.predict_generator(predict_it, steps=24)
loss, acc  = model.evaluate_generator(test_it)
print("Accuracy: {:5.2f}%".format(100 * acc))


# Checking label distributions
Y = train_it.classes
Y_pred = model.predict_generator(test_it)
Y_pred = np.argmax(Y_pred, axis=1)
Y_test = test_it.classes
print(f'Y_train label distrib: {np.unique(Y, return_counts=True)}')
print(f'Y_test label distrib: {np.unique(Y_test, return_counts=True)}')
y_pred_distrib = np.unique(Y_pred, return_counts=True)
print(f'Y_pred_label distrib: {y_pred_distrib}')
np.save('xceptionNet/Y_pred_label_distrib_patchsize299.npy', y_pred_distrib)
print(f'Number of different predicted labels: {np.shape(np.unique(Y_pred, return_counts=True))}')


# Confusion Matrix
print(f"Accuracy: {accuracy_score(Y_test, Y_pred)}")
print(f"Confusion matrix: \n{confusion_matrix(Y_test, Y_pred)}")
np.save('xceptionNet/confusion_matrix_patchsize299.npy', confusion_matrix(Y_test, Y_pred))
print(f"Classification Report:\n {classification_report(Y_test, Y_pred)}")

model.save('xceptionNet/allchannel_patches100_classes5_patchsize299.h5')    


# TODOS:
# set layers in pre-trained model as trainable ?
# Use imagenet-pretrained model
# Increase number of classes from 5
# try out different batch-size and epochs
# save model_weights, train_dict, final_model
