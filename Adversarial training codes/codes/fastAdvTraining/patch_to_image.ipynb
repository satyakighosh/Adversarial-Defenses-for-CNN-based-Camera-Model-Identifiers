{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import apex.amp as amp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision.models import densenet121 as _densenet\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "import apex.amp as amp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.feature_extraction import image\n",
    "from PIL import Image\n",
    "\n",
    "cuda:1\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "print(torch.cuda.device_count())\n",
    "#torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "lr_max = 0.004\n",
    "momentum = 0.9\n",
    "num_classes = 5\n",
    "model_name = \"densenet\"\n",
    "feature_extract = False\n",
    "root_dir = '/home/mtech/2020/satyaki_ghosh/datasets/VISION'\n",
    "IMG_SIZE = 224\n",
    "channels = 3\n",
    "patches_per_photo = 2\n",
    "\n",
    "\n",
    "vision_mean = (0.4333, 0.4635, 0.4767)\n",
    "vision_std = (0.2687, 0.2536, 0.2593)\n",
    "mu = torch.tensor(vision_mean).view(3,1,1).cuda()\n",
    "std = torch.tensor(vision_std).view(3,1,1).cuda()\n",
    "epsilon = (8 / 255.) / std\n",
    "pgd_alpha = (2 / 255.) / std\n",
    "upper_limit = ((1 - mu)/ std)\n",
    "lower_limit = ((0 - mu)/ std) \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(X, lower_limit, upper_limit):\n",
    "    return torch.max(torch.min(X, upper_limit), lower_limit)\n",
    "\n",
    "def attack_pgd(model, X, y, epsilon, alpha, attack_iters, restarts, opt=None):\n",
    "    max_loss = torch.zeros_like(y).cuda()\n",
    "    max_delta = torch.zeros_like(X).cuda()\n",
    "    for zz in range(restarts):\n",
    "        delta = torch.zeros_like(X).cuda()\n",
    "        for i in range(len(epsilon)):\n",
    "            delta[:, i, :, :].uniform_(-epsilon[i][0][0].item(), epsilon[i][0][0].item())\n",
    "        delta.data = clamp(delta, lower_limit - X, upper_limit - X)\n",
    "        delta.requires_grad = True\n",
    "        for _ in range(attack_iters):\n",
    "            output = model(X + delta)\n",
    "            index = torch.where(output.max(1)[1] == y)\n",
    "            if len(index[0]) == 0:\n",
    "                break\n",
    "            loss = criterion(output, y)\n",
    "            if opt is not None:\n",
    "                with amp.scale_loss(loss, opt) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            grad = delta.grad.detach()\n",
    "            d = delta[index[0], :, :, :]\n",
    "            g = grad[index[0], :, :, :]\n",
    "            d = clamp(d + alpha * torch.sign(g), -epsilon, epsilon)\n",
    "            d = clamp(d, lower_limit - X[index[0], :, :, :], upper_limit - X[index[0], :, :, :])\n",
    "            delta.data[index[0], :, :, :] = d\n",
    "            delta.grad.zero_()\n",
    "\n",
    "        all_loss = F.cross_entropy(model(X+delta), y, reduction='none').detach()\n",
    "        max_delta[all_loss >= max_loss] = delta.detach()[all_loss >= max_loss]\n",
    "        max_loss = torch.max(max_loss.float(), all_loss)\n",
    "    return max_delta\n",
    "\n",
    "def evaluate_pgd(X, y, model, attack_iters, restarts):\n",
    "    epsilon = (8 / 255.) / std   \n",
    "    alpha = (2 / 255.) / std\n",
    "    pgd_loss = 0\n",
    "    pgd_acc = 0\n",
    "    n = 0\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    X, y = X.cuda(), y.cuda()\n",
    "    pgd_delta = attack_pgd(model, X, y, epsilon, alpha, attack_iters, restarts)\n",
    "    with torch.no_grad():\n",
    "        output = model(X + pgd_delta)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        pgd_loss += loss.item() * y.size(0)\n",
    "        pgd_acc += (output.max(1)[1] == y).sum().item()\n",
    "        n += y.size(0)\n",
    "    return pgd_loss/n, pgd_acc/n, output\n",
    "\n",
    "def evaluate_standard(X, y, model):\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    n = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        output = model(X)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        test_loss += loss.item() * y.size(0)\n",
    "        test_acc += (output.max(1)[1] == y).sum().item()\n",
    "        n += y.size(0)\n",
    "    return test_loss/n, test_acc/n, output\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    \"\"\" Densenet\n",
    "    \"\"\"\n",
    "    model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.classifier.in_features\n",
    "    model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "    input_size = 224\n",
    "\n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6958981\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "\n",
    "model, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = '/home/mtech/2020/satyaki_ghosh/codes/fastAdvTraining/fast_adversarial/VISION/train_free_output/model_77.pth'\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.float()\n",
    "model.eval()\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████▏                                            | 23/50 [05:55<08:11, 18.20s/it, adv_img_acc=0, adv_patch_acc=0, img_acc=0, patch_acc=0]Corrupt JPEG data: premature end of data segment\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [13:09<00:00, 15.78s/it, adv_img_acc=0, adv_patch_acc=0, img_acc=0, patch_acc=0]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50/50 [12:49<00:00, 15.40s/it, adv_img_acc=0.5, adv_patch_acc=0.477, img_acc=0.5, patch_acc=0.477]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50/50 [12:30<00:00, 15.00s/it, adv_img_acc=0.34, adv_patch_acc=0.34, img_acc=0.34, patch_acc=0.34]\n",
      "100%|███████████████████████████████████████████████████████████████████| 50/50 [12:27<00:00, 14.94s/it, adv_img_acc=0.255, adv_patch_acc=0.255, img_acc=0.255, patch_acc=0.255]\n",
      "100%|███████████████████████████████████████████████████████████████████| 50/50 [10:15<00:00, 12.32s/it, adv_img_acc=0.204, adv_patch_acc=0.204, img_acc=0.204, patch_acc=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-level normal accuracy: 0.204\n",
      "Patch-level normal accuracy: 0.20388\n",
      "\n",
      "Image-level robust accuracy: 0.204\n",
      "Patch-level robust accuracy: 0.20388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "patches_per_photo = 100\n",
    "images_per_model = 50\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"D01_Samsung_GalaxyS3Mini\",\n",
    "    \"D02_Apple_iPhone4s\",\n",
    "    \"D03_Huawei_P9\",\n",
    "    \"D04_LG_D290\",\n",
    "    \"D05_Apple_iPhone5c\"]\n",
    "\n",
    "total = 0\n",
    "correct_images_normal = 0\n",
    "correct_patches_normal = 0\n",
    "correct_images_adv = 0\n",
    "correct_patches_adv = 0\n",
    "\n",
    "\n",
    "for camera_model in CATEGORIES:\n",
    "    y = CATEGORIES.index(camera_model)\n",
    "    path = os.path.join(os.path.join(root_dir, camera_model), 'images/nat')\n",
    "    \n",
    "    # shuffling the images to avoid similar/close images\n",
    "    photos = os.listdir(path)\n",
    "    random.shuffle(photos)\n",
    "    test_images = photos[0:images_per_model]\n",
    "    \n",
    "    loop = tqdm(test_images, total = len(test_images))\n",
    "    for image_name in loop:\n",
    "        total += 1\n",
    "        image_path = os.path.join(path, image_name)\n",
    "        img = cv2.imread(image_path)\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "\n",
    "        pgd_preds = []\n",
    "        std_preds = []\n",
    "\n",
    "        patches = image.extract_patches_2d(img, (IMG_SIZE, IMG_SIZE), max_patches = patches_per_photo)\n",
    "        y = torch.tensor(y, dtype=torch.long).reshape(1).cuda()\n",
    "\n",
    "        for i in range(patches_per_photo) :                    \n",
    "            X = patches[i]            \n",
    "#             im = Image.fromarray(X)\n",
    "#             plt.imshow(im)\n",
    "#             plt.show()\n",
    "            X = X.astype('float')\n",
    "            X = torch.from_numpy(X)\n",
    "            X = X.permute(2, 0, 1)\n",
    "            X = X.reshape(-1,3,224,224).float()\n",
    "            test_loss, test_acc, std_output = evaluate_standard(X, y, model)\n",
    "            std_preds.append(std_output.max(1)[1])\n",
    "            # pgd_loss, pgd_acc, pgd_output = evaluate_pgd(X, y, model, 50, 10)\n",
    "            pgd_loss, pgd_acc, pgd_output = test_loss, test_acc, std_output\n",
    "            pgd_preds.append(pgd_output.max(1)[1])\n",
    "            if y == std_output.max(1)[1]:\n",
    "                correct_patches_normal = correct_patches_normal + 1\n",
    "            if y[0] == pgd_output.max(1)[1][0]:\n",
    "                correct_patches_adv = correct_patches_adv + 1\n",
    "            # print(f'Original label = {y[0]}, Standard Predicted label = {std_output.max(1)[1][0]}, PGD Predicted label = {pgd_output.max(1)[1][0]}') \n",
    "        \n",
    "        normal_output = max(set(std_preds), key=std_preds.count)[0]\n",
    "        adv_output = max(set(pgd_preds), key=pgd_preds.count)[0]\n",
    "        \n",
    "        if normal_output == y[0]:\n",
    "            correct_images_normal = correct_images_normal + 1\n",
    "        if adv_output == y[0]:\n",
    "            correct_images_adv = correct_images_adv + 1            \n",
    "    \n",
    "        loop.set_postfix(img_acc = correct_images_normal / total,\n",
    "                         patch_acc = correct_patches_normal / (total * patches_per_photo),\n",
    "                         adv_img_acc = correct_images_adv / total,\n",
    "                         adv_patch_acc = correct_patches_adv / (total * patches_per_photo)\n",
    "                        )\n",
    "\n",
    "#         print(f'Overall output of normal image (by simple voting) : {normal_output}')\n",
    "#         print(f'Overall output of adversarial image (by simple voting) : {adv_output}')\n",
    "\n",
    "print(f'Image-level normal accuracy: {correct_images_normal / total}')\n",
    "print(f'Patch-level normal accuracy: {correct_patches_normal / (total * patches_per_photo)}')\n",
    "print()\n",
    "print(f'Image-level robust accuracy: {correct_images_adv / total}')\n",
    "print(f'Patch-level robust accuracy: {correct_patches_adv / (total * patches_per_photo)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = torch.zeros_like(X, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0940,  0.0841, -0.0643,  ...,  0.0049,  0.0987, -0.0890],\n",
       "          [ 0.0105, -0.0587, -0.0486,  ..., -0.0997, -0.0105, -0.0616],\n",
       "          [-0.0018, -0.0839,  0.0974,  ...,  0.0002, -0.0747, -0.0394],\n",
       "          ...,\n",
       "          [-0.0060,  0.0578,  0.0990,  ..., -0.0624, -0.0206,  0.0895],\n",
       "          [ 0.0358, -0.0337,  0.0994,  ..., -0.0994,  0.0875,  0.0292],\n",
       "          [-0.0589,  0.0325,  0.0957,  ...,  0.0313, -0.0214, -0.0574]],\n",
       "\n",
       "         [[ 0.0416,  0.0290, -0.0298,  ...,  0.0935,  0.0965, -0.0036],\n",
       "          [-0.0107,  0.0099,  0.0026,  ..., -0.0475,  0.0270, -0.0738],\n",
       "          [ 0.0766, -0.0156, -0.0716,  ...,  0.0185, -0.0824,  0.0350],\n",
       "          ...,\n",
       "          [ 0.0516,  0.0168, -0.0805,  ..., -0.0767, -0.0673, -0.0529],\n",
       "          [ 0.0164, -0.0160, -0.0567,  ...,  0.0893,  0.0451,  0.0888],\n",
       "          [ 0.0271,  0.0580, -0.0780,  ...,  0.0712, -0.0442,  0.0213]],\n",
       "\n",
       "         [[-0.0081,  0.0582,  0.0163,  ...,  0.0631,  0.0589, -0.0511],\n",
       "          [-0.0422,  0.0237, -0.0590,  ...,  0.0520,  0.0011, -0.0750],\n",
       "          [ 0.0921,  0.0413,  0.0722,  ..., -0.0846,  0.0021, -0.0730],\n",
       "          ...,\n",
       "          [ 0.0581,  0.0769,  0.0768,  ..., -0.0469, -0.0096, -0.0731],\n",
       "          [ 0.0811,  0.0690, -0.0928,  ..., -0.0967,  0.0541, -0.0630],\n",
       "          [-0.0876,  0.0729,  0.0247,  ..., -0.0809,  0.0841,  0.0990]]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz[:,:,:].uniform_(-0.1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.reshape(-1,3,224,224).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def patch_and_image_level_accuracy(model):\n",
    "    patches_per_photo = 50\n",
    "    images_per_model = 5\n",
    "\n",
    "    CATEGORIES = [\n",
    "        \"D01_Samsung_GalaxyS3Mini\",\n",
    "        \"D02_Apple_iPhone4s\",\n",
    "        \"D03_Huawei_P9\",\n",
    "        \"D04_LG_D290\",\n",
    "        \"D05_Apple_iPhone5c\"]\n",
    "\n",
    "    total = 0\n",
    "    correct_images_normal = 0\n",
    "    correct_patches_normal = 0\n",
    "    correct_images_adv = 0\n",
    "    correct_patches_adv = 0\n",
    "\n",
    "\n",
    "    for camera_model in CATEGORIES:\n",
    "        y = CATEGORIES.index(camera_model)\n",
    "        path = os.path.join(os.path.join(root_dir, camera_model), 'images/nat')\n",
    "\n",
    "        # shuffling the images to avoid similar/close images\n",
    "        photos = os.listdir(path)\n",
    "        random.shuffle(photos)\n",
    "        test_images = photos[0:images_per_model]\n",
    "\n",
    "        loop = tqdm(test_images, total = len(test_images))\n",
    "        for image_name in loop:\n",
    "            total += 1\n",
    "            image_path = os.path.join(path, image_name)\n",
    "            img = cv2.imread(image_path)\n",
    "\n",
    "            pgd_preds = []\n",
    "            std_preds = []\n",
    "\n",
    "            patches = image.extract_patches_2d(img, (IMG_SIZE, IMG_SIZE), max_patches = patches_per_photo)\n",
    "            y = torch.tensor(y, dtype=torch.long).reshape(1).cuda()\n",
    "\n",
    "            for i in range(patches_per_photo) :                    \n",
    "                X = patches[i]            \n",
    "                X = X.astype('float')\n",
    "                X = torch.from_numpy(X)\n",
    "                X = X.permute(2, 0, 1)\n",
    "                X = X.reshape(-1,3,224,224).float()\n",
    "                test_loss, test_acc, std_output = evaluate_standard(X, y, model)\n",
    "                std_preds.append(std_output.max(1)[1])\n",
    "                pgd_loss, pgd_acc, pgd_output = evaluate_pgd(X, y, model, 50, 10)\n",
    "                pgd_preds.append(pgd_output.max(1)[1])\n",
    "                if y == std_output.max(1)[1]:\n",
    "                    correct_patches_normal = correct_patches_normal + 1\n",
    "                if y[0] == pgd_output.max(1)[1][0]:\n",
    "                    correct_patches_adv = correct_patches_adv + 1\n",
    "                    \n",
    "            normal_output = max(set(std_preds), key=std_preds.count)[0]\n",
    "            adv_output = max(set(pgd_preds), key=pgd_preds.count)[0]\n",
    "\n",
    "            if normal_output == y[0]:\n",
    "                correct_images_normal = correct_images_normal + 1\n",
    "            if adv_output == y[0]:\n",
    "                correct_images_adv = correct_images_adv + 1            \n",
    "\n",
    "            loop.set_postfix(img_acc = correct_images_normal / total,\n",
    "                             patch_acc = correct_patches_normal / (total * patches_per_photo),\n",
    "                             adv_img_acc = correct_images_adv / total,\n",
    "                             adv_patch_acc = correct_patches_adv / (total * patches_per_photo)\n",
    "                            )\n",
    "            \n",
    "    print(f'Image-level normal accuracy: {correct_images_normal / total}')\n",
    "    print(f'Patch-level normal accuracy: {correct_patches_normal / (total * patches_per_photo)}')\n",
    "    print()\n",
    "    print(f'Image-level robust accuracy: {correct_images_adv / total}')\n",
    "    print(f'Patch-level robust accuracy: {correct_patches_adv / (total * patches_per_photo)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
