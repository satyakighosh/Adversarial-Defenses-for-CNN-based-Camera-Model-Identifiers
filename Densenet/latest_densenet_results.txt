Train on 47250 samples, validate on 5250 samples
Epoch 1/80
2022-04-05 14:28:23.996971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-04-05 14:28:24.707474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
47232/47250 [============================>.] - ETA: 0s - loss: 3.0130 - sparse_categorical_accuracy: 0.4402      
Epoch 00001: saving model to training_1/cp_0001.ckpt
47250/47250 [==============================] - 274s 6ms/sample - loss: 3.0130 - sparse_categorical_accuracy: 0.4402 - val_loss: 3.0733 - val_sparse_categorical_accuracy: 0.4341
Epoch 2/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.8404 - sparse_categorical_accuracy: 0.5282  
Epoch 00002: saving model to training_1/cp_0002.ckpt
47250/47250 [==============================] - 232s 5ms/sample - loss: 2.8403 - sparse_categorical_accuracy: 0.5283 - val_loss: 2.9930 - val_sparse_categorical_accuracy: 0.4670
Epoch 3/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.7410 - sparse_categorical_accuracy: 0.5729  
Epoch 00003: saving model to training_1/cp_0003.ckpt
47250/47250 [==============================] - 232s 5ms/sample - loss: 2.7411 - sparse_categorical_accuracy: 0.5729 - val_loss: 2.6655 - val_sparse_categorical_accuracy: 0.6120
Epoch 4/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.6482 - sparse_categorical_accuracy: 0.6161  
Epoch 00004: saving model to training_1/cp_0004.ckpt
47250/47250 [==============================] - 231s 5ms/sample - loss: 2.6482 - sparse_categorical_accuracy: 0.6161 - val_loss: 2.8742 - val_sparse_categorical_accuracy: 0.5505
Epoch 5/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.5640 - sparse_categorical_accuracy: 0.6517   
Epoch 00005: saving model to training_1/cp_0005.ckpt
47250/47250 [==============================] - 231s 5ms/sample - loss: 2.5643 - sparse_categorical_accuracy: 0.6516 - val_loss: 2.6094 - val_sparse_categorical_accuracy: 0.6316
Epoch 6/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.4971 - sparse_categorical_accuracy: 0.6781  
Epoch 00006: saving model to training_1/cp_0006.ckpt
47250/47250 [==============================] - 231s 5ms/sample - loss: 2.4974 - sparse_categorical_accuracy: 0.6779 - val_loss: 3.4958 - val_sparse_categorical_accuracy: 0.4756
Epoch 7/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.4475 - sparse_categorical_accuracy: 0.6977  
Epoch 00007: saving model to training_1/cp_0007.ckpt
47250/47250 [==============================] - 222s 5ms/sample - loss: 2.4476 - sparse_categorical_accuracy: 0.6977 - val_loss: 2.6984 - val_sparse_categorical_accuracy: 0.6204
Epoch 8/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.3949 - sparse_categorical_accuracy: 0.7200  
Epoch 00008: saving model to training_1/cp_0008.ckpt
47250/47250 [==============================] - 264s 6ms/sample - loss: 2.3948 - sparse_categorical_accuracy: 0.7200 - val_loss: 3.8918 - val_sparse_categorical_accuracy: 0.4966
Epoch 9/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.3592 - sparse_categorical_accuracy: 0.7344  
Epoch 00009: saving model to training_1/cp_0009.ckpt
47250/47250 [==============================] - 233s 5ms/sample - loss: 2.3594 - sparse_categorical_accuracy: 0.7343 - val_loss: 3.6611 - val_sparse_categorical_accuracy: 0.4682
Epoch 10/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.3228 - sparse_categorical_accuracy: 0.7474  
Epoch 00010: saving model to training_1/cp_0010.ckpt
47250/47250 [==============================] - 237s 5ms/sample - loss: 2.3228 - sparse_categorical_accuracy: 0.7473 - val_loss: 2.7518 - val_sparse_categorical_accuracy: 0.6349
Epoch 11/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.2918 - sparse_categorical_accuracy: 0.7601  
Epoch 00011: saving model to training_1/cp_0011.ckpt
47250/47250 [==============================] - 291s 6ms/sample - loss: 2.2917 - sparse_categorical_accuracy: 0.7602 - val_loss: 3.2380 - val_sparse_categorical_accuracy: 0.5648
Epoch 12/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.2616 - sparse_categorical_accuracy: 0.7707  
Epoch 00012: saving model to training_1/cp_0012.ckpt
47250/47250 [==============================] - 352s 7ms/sample - loss: 2.2616 - sparse_categorical_accuracy: 0.7707 - val_loss: 4.8502 - val_sparse_categorical_accuracy: 0.3891
Epoch 13/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.2416 - sparse_categorical_accuracy: 0.7770  
Epoch 00013: saving model to training_1/cp_0013.ckpt
47250/47250 [==============================] - 297s 6ms/sample - loss: 2.2416 - sparse_categorical_accuracy: 0.7770 - val_loss: 2.3871 - val_sparse_categorical_accuracy: 0.7221
Epoch 14/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.2152 - sparse_categorical_accuracy: 0.7891  
Epoch 00014: saving model to training_1/cp_0014.ckpt
47250/47250 [==============================] - 266s 6ms/sample - loss: 2.2152 - sparse_categorical_accuracy: 0.7891 - val_loss: 4.7216 - val_sparse_categorical_accuracy: 0.6370
Epoch 15/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.1942 - sparse_categorical_accuracy: 0.7937  
Epoch 00015: saving model to training_1/cp_0015.ckpt
47250/47250 [==============================] - 269s 6ms/sample - loss: 2.1942 - sparse_categorical_accuracy: 0.7937 - val_loss: 3.4067 - val_sparse_categorical_accuracy: 0.6021
Epoch 16/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.1804 - sparse_categorical_accuracy: 0.8008  
Epoch 00016: saving model to training_1/cp_0016.ckpt
47250/47250 [==============================] - 254s 5ms/sample - loss: 2.1804 - sparse_categorical_accuracy: 0.8008 - val_loss: 3.2036 - val_sparse_categorical_accuracy: 0.6617
Epoch 17/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.1594 - sparse_categorical_accuracy: 0.8061  
Epoch 00017: saving model to training_1/cp_0017.ckpt
47250/47250 [==============================] - 270s 6ms/sample - loss: 2.1593 - sparse_categorical_accuracy: 0.8061 - val_loss: 3.1744 - val_sparse_categorical_accuracy: 0.6364
Epoch 18/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.1396 - sparse_categorical_accuracy: 0.8159  
Epoch 00018: saving model to training_1/cp_0018.ckpt
47250/47250 [==============================] - 292s 6ms/sample - loss: 2.1395 - sparse_categorical_accuracy: 0.8159 - val_loss: 4.1116 - val_sparse_categorical_accuracy: 0.5204
Epoch 19/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.1180 - sparse_categorical_accuracy: 0.8238  
Epoch 00019: saving model to training_1/cp_0019.ckpt
47250/47250 [==============================] - 291s 6ms/sample - loss: 2.1179 - sparse_categorical_accuracy: 0.8239 - val_loss: 3.6732 - val_sparse_categorical_accuracy: 0.5924
Epoch 20/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.1081 - sparse_categorical_accuracy: 0.8266  
Epoch 00020: saving model to training_1/cp_0020.ckpt
47250/47250 [==============================] - 292s 6ms/sample - loss: 2.1082 - sparse_categorical_accuracy: 0.8266 - val_loss: 4.0208 - val_sparse_categorical_accuracy: 0.5625
Epoch 21/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.0919 - sparse_categorical_accuracy: 0.8321  
Epoch 00021: saving model to training_1/cp_0021.ckpt
47250/47250 [==============================] - 262s 6ms/sample - loss: 2.0919 - sparse_categorical_accuracy: 0.8320 - val_loss: 4.2952 - val_sparse_categorical_accuracy: 0.5743
Epoch 22/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.0812 - sparse_categorical_accuracy: 0.8349  
Epoch 00022: saving model to training_1/cp_0022.ckpt
47250/47250 [==============================] - 303s 6ms/sample - loss: 2.0812 - sparse_categorical_accuracy: 0.8349 - val_loss: 3.6949 - val_sparse_categorical_accuracy: 0.6211
Epoch 23/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.0628 - sparse_categorical_accuracy: 0.8423  
Epoch 00023: saving model to training_1/cp_0023.ckpt
47250/47250 [==============================] - 331s 7ms/sample - loss: 2.0628 - sparse_categorical_accuracy: 0.8423 - val_loss: 4.4270 - val_sparse_categorical_accuracy: 0.5122
Epoch 24/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.0516 - sparse_categorical_accuracy: 0.8473  
Epoch 00024: saving model to training_1/cp_0024.ckpt
47250/47250 [==============================] - 296s 6ms/sample - loss: 2.0517 - sparse_categorical_accuracy: 0.8473 - val_loss: 2.4408 - val_sparse_categorical_accuracy: 0.7392
Epoch 25/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.0399 - sparse_categorical_accuracy: 0.8522  
Epoch 00025: saving model to training_1/cp_0025.ckpt
47250/47250 [==============================] - 262s 6ms/sample - loss: 2.0399 - sparse_categorical_accuracy: 0.8522 - val_loss: 2.5305 - val_sparse_categorical_accuracy: 0.7211
Epoch 26/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.0272 - sparse_categorical_accuracy: 0.8549  
Epoch 00026: saving model to training_1/cp_0026.ckpt
47250/47250 [==============================] - 257s 5ms/sample - loss: 2.0272 - sparse_categorical_accuracy: 0.8549 - val_loss: 3.8585 - val_sparse_categorical_accuracy: 0.5417
Epoch 27/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.0203 - sparse_categorical_accuracy: 0.8574  
Epoch 00027: saving model to training_1/cp_0027.ckpt
47250/47250 [==============================] - 260s 5ms/sample - loss: 2.0203 - sparse_categorical_accuracy: 0.8574 - val_loss: 2.0417 - val_sparse_categorical_accuracy: 0.8545
Epoch 28/80
47232/47250 [============================>.] - ETA: 0s - loss: 2.0082 - sparse_categorical_accuracy: 0.8603  
Epoch 00028: saving model to training_1/cp_0028.ckpt
47250/47250 [==============================] - 259s 5ms/sample - loss: 2.0083 - sparse_categorical_accuracy: 0.8603 - val_loss: 2.4395 - val_sparse_categorical_accuracy: 0.7507
Epoch 29/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9998 - sparse_categorical_accuracy: 0.8634  
Epoch 00029: saving model to training_1/cp_0029.ckpt
47250/47250 [==============================] - 251s 5ms/sample - loss: 2.0001 - sparse_categorical_accuracy: 0.8633 - val_loss: 1.9388 - val_sparse_categorical_accuracy: 0.8888
Epoch 30/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9945 - sparse_categorical_accuracy: 0.8666  
Epoch 00030: saving model to training_1/cp_0030.ckpt
47250/47250 [==============================] - 256s 5ms/sample - loss: 1.9945 - sparse_categorical_accuracy: 0.8666 - val_loss: 3.6776 - val_sparse_categorical_accuracy: 0.6088
Epoch 31/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9798 - sparse_categorical_accuracy: 0.8732  
Epoch 00031: saving model to training_1/cp_0031.ckpt
47250/47250 [==============================] - 256s 5ms/sample - loss: 1.9799 - sparse_categorical_accuracy: 0.8732 - val_loss: 3.9149 - val_sparse_categorical_accuracy: 0.6291
Epoch 32/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9773 - sparse_categorical_accuracy: 0.8724  
Epoch 00032: saving model to training_1/cp_0032.ckpt
47250/47250 [==============================] - 252s 5ms/sample - loss: 1.9773 - sparse_categorical_accuracy: 0.8724 - val_loss: 3.7271 - val_sparse_categorical_accuracy: 0.6402
Epoch 33/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9689 - sparse_categorical_accuracy: 0.8744  
Epoch 00033: saving model to training_1/cp_0033.ckpt
47250/47250 [==============================] - 232s 5ms/sample - loss: 1.9690 - sparse_categorical_accuracy: 0.8743 - val_loss: 2.2424 - val_sparse_categorical_accuracy: 0.8080
Epoch 34/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9635 - sparse_categorical_accuracy: 0.8766  
Epoch 00034: saving model to training_1/cp_0034.ckpt
47250/47250 [==============================] - 267s 6ms/sample - loss: 1.9635 - sparse_categorical_accuracy: 0.8766 - val_loss: 3.5586 - val_sparse_categorical_accuracy: 0.6282
Epoch 35/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9543 - sparse_categorical_accuracy: 0.8793  
Epoch 00035: saving model to training_1/cp_0035.ckpt
47250/47250 [==============================] - 275s 6ms/sample - loss: 1.9543 - sparse_categorical_accuracy: 0.8793 - val_loss: 2.4733 - val_sparse_categorical_accuracy: 0.7756
Epoch 36/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9510 - sparse_categorical_accuracy: 0.8815  
Epoch 00036: saving model to training_1/cp_0036.ckpt
47250/47250 [==============================] - 287s 6ms/sample - loss: 1.9510 - sparse_categorical_accuracy: 0.8815 - val_loss: 2.5974 - val_sparse_categorical_accuracy: 0.7524
Epoch 37/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9463 - sparse_categorical_accuracy: 0.8835  
Epoch 00037: saving model to training_1/cp_0037.ckpt
47250/47250 [==============================] - 322s 7ms/sample - loss: 1.9464 - sparse_categorical_accuracy: 0.8835 - val_loss: 2.3115 - val_sparse_categorical_accuracy: 0.7745
Epoch 38/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9380 - sparse_categorical_accuracy: 0.8873  
Epoch 00038: saving model to training_1/cp_0038.ckpt
47250/47250 [==============================] - 310s 7ms/sample - loss: 1.9380 - sparse_categorical_accuracy: 0.8873 - val_loss: 2.4395 - val_sparse_categorical_accuracy: 0.7665
Epoch 39/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9363 - sparse_categorical_accuracy: 0.8885  
Epoch 00039: saving model to training_1/cp_0039.ckpt
47250/47250 [==============================] - 283s 6ms/sample - loss: 1.9363 - sparse_categorical_accuracy: 0.8885 - val_loss: 2.1528 - val_sparse_categorical_accuracy: 0.8154
Epoch 40/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9335 - sparse_categorical_accuracy: 0.8876  
Epoch 00040: saving model to training_1/cp_0040.ckpt
47250/47250 [==============================] - 273s 6ms/sample - loss: 1.9337 - sparse_categorical_accuracy: 0.8875 - val_loss: 3.1968 - val_sparse_categorical_accuracy: 0.6069
Epoch 41/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9245 - sparse_categorical_accuracy: 0.8900  
Epoch 00041: saving model to training_1/cp_0041.ckpt
47250/47250 [==============================] - 321s 7ms/sample - loss: 1.9247 - sparse_categorical_accuracy: 0.8899 - val_loss: 3.3240 - val_sparse_categorical_accuracy: 0.7006
Epoch 42/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9279 - sparse_categorical_accuracy: 0.8894  
Epoch 00042: saving model to training_1/cp_0042.ckpt
47250/47250 [==============================] - 301s 6ms/sample - loss: 1.9279 - sparse_categorical_accuracy: 0.8895 - val_loss: 3.1280 - val_sparse_categorical_accuracy: 0.7036
Epoch 43/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9223 - sparse_categorical_accuracy: 0.8921  
Epoch 00043: saving model to training_1/cp_0043.ckpt
47250/47250 [==============================] - 250s 5ms/sample - loss: 1.9222 - sparse_categorical_accuracy: 0.8922 - val_loss: 3.0048 - val_sparse_categorical_accuracy: 0.6450
Epoch 44/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9134 - sparse_categorical_accuracy: 0.8953  
Epoch 00044: saving model to training_1/cp_0044.ckpt
47250/47250 [==============================] - 233s 5ms/sample - loss: 1.9134 - sparse_categorical_accuracy: 0.8953 - val_loss: 2.3431 - val_sparse_categorical_accuracy: 0.7853
Epoch 45/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9118 - sparse_categorical_accuracy: 0.8942  
Epoch 00045: saving model to training_1/cp_0045.ckpt
47250/47250 [==============================] - 229s 5ms/sample - loss: 1.9120 - sparse_categorical_accuracy: 0.8941 - val_loss: 2.2418 - val_sparse_categorical_accuracy: 0.8181
Epoch 46/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9053 - sparse_categorical_accuracy: 0.8986  
Epoch 00046: saving model to training_1/cp_0046.ckpt
47250/47250 [==============================] - 229s 5ms/sample - loss: 1.9053 - sparse_categorical_accuracy: 0.8985 - val_loss: 4.4492 - val_sparse_categorical_accuracy: 0.5851
Epoch 47/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9051 - sparse_categorical_accuracy: 0.8970  
Epoch 00047: saving model to training_1/cp_0047.ckpt
47250/47250 [==============================] - 229s 5ms/sample - loss: 1.9054 - sparse_categorical_accuracy: 0.8970 - val_loss: 2.6403 - val_sparse_categorical_accuracy: 0.7491
Epoch 48/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.9060 - sparse_categorical_accuracy: 0.8974  
Epoch 00048: saving model to training_1/cp_0048.ckpt
47250/47250 [==============================] - 230s 5ms/sample - loss: 1.9061 - sparse_categorical_accuracy: 0.8974 - val_loss: 1.8517 - val_sparse_categorical_accuracy: 0.9152
Epoch 49/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8977 - sparse_categorical_accuracy: 0.9008  
Epoch 00049: saving model to training_1/cp_0049.ckpt
47250/47250 [==============================] - 269s 6ms/sample - loss: 1.8977 - sparse_categorical_accuracy: 0.9008 - val_loss: 2.4664 - val_sparse_categorical_accuracy: 0.7730
Epoch 50/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8913 - sparse_categorical_accuracy: 0.9031  
Epoch 00050: saving model to training_1/cp_0050.ckpt
47250/47250 [==============================] - 330s 7ms/sample - loss: 1.8914 - sparse_categorical_accuracy: 0.9031 - val_loss: 2.2721 - val_sparse_categorical_accuracy: 0.7771
Epoch 51/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8977 - sparse_categorical_accuracy: 0.8997  
Epoch 00051: saving model to training_1/cp_0051.ckpt
47250/47250 [==============================] - 387s 8ms/sample - loss: 1.8977 - sparse_categorical_accuracy: 0.8997 - val_loss: 2.8523 - val_sparse_categorical_accuracy: 0.7421
Epoch 52/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8846 - sparse_categorical_accuracy: 0.9054  
Epoch 00052: saving model to training_1/cp_0052.ckpt
47250/47250 [==============================] - 379s 8ms/sample - loss: 1.8846 - sparse_categorical_accuracy: 0.9054 - val_loss: 1.8943 - val_sparse_categorical_accuracy: 0.8977
Epoch 53/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8862 - sparse_categorical_accuracy: 0.9032  
Epoch 00053: saving model to training_1/cp_0053.ckpt
47250/47250 [==============================] - 382s 8ms/sample - loss: 1.8863 - sparse_categorical_accuracy: 0.9032 - val_loss: 2.1055 - val_sparse_categorical_accuracy: 0.8448
Epoch 54/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8824 - sparse_categorical_accuracy: 0.9057  
Epoch 00054: saving model to training_1/cp_0054.ckpt
47250/47250 [==============================] - 382s 8ms/sample - loss: 1.8824 - sparse_categorical_accuracy: 0.9057 - val_loss: 2.2882 - val_sparse_categorical_accuracy: 0.8229
Epoch 55/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8770 - sparse_categorical_accuracy: 0.9080  
Epoch 00055: saving model to training_1/cp_0055.ckpt
47250/47250 [==============================] - 378s 8ms/sample - loss: 1.8770 - sparse_categorical_accuracy: 0.9080 - val_loss: 2.0279 - val_sparse_categorical_accuracy: 0.8627
Epoch 56/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8802 - sparse_categorical_accuracy: 0.9063  
Epoch 00056: saving model to training_1/cp_0056.ckpt
47250/47250 [==============================] - 381s 8ms/sample - loss: 1.8802 - sparse_categorical_accuracy: 0.9063 - val_loss: 2.3009 - val_sparse_categorical_accuracy: 0.8072
Epoch 57/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8754 - sparse_categorical_accuracy: 0.9075  
Epoch 00057: saving model to training_1/cp_0057.ckpt
47250/47250 [==============================] - 376s 8ms/sample - loss: 1.8754 - sparse_categorical_accuracy: 0.9075 - val_loss: 1.8454 - val_sparse_categorical_accuracy: 0.9217
Epoch 58/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8711 - sparse_categorical_accuracy: 0.9097  
Epoch 00058: saving model to training_1/cp_0058.ckpt
47250/47250 [==============================] - 377s 8ms/sample - loss: 1.8714 - sparse_categorical_accuracy: 0.9097 - val_loss: 2.3880 - val_sparse_categorical_accuracy: 0.7958
Epoch 59/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8694 - sparse_categorical_accuracy: 0.9094  
Epoch 00059: saving model to training_1/cp_0059.ckpt
47250/47250 [==============================] - 378s 8ms/sample - loss: 1.8693 - sparse_categorical_accuracy: 0.9094 - val_loss: 4.0360 - val_sparse_categorical_accuracy: 0.6661
Epoch 60/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8660 - sparse_categorical_accuracy: 0.9107  
Epoch 00060: saving model to training_1/cp_0060.ckpt
47250/47250 [==============================] - 381s 8ms/sample - loss: 1.8660 - sparse_categorical_accuracy: 0.9107 - val_loss: 2.2242 - val_sparse_categorical_accuracy: 0.8139
Epoch 61/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8699 - sparse_categorical_accuracy: 0.9089  
Epoch 00061: saving model to training_1/cp_0061.ckpt
47250/47250 [==============================] - 379s 8ms/sample - loss: 1.8699 - sparse_categorical_accuracy: 0.9089 - val_loss: 1.9248 - val_sparse_categorical_accuracy: 0.8832
Epoch 62/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8628 - sparse_categorical_accuracy: 0.9124  
Epoch 00062: saving model to training_1/cp_0062.ckpt
47250/47250 [==============================] - 226s 5ms/sample - loss: 1.8627 - sparse_categorical_accuracy: 0.9124 - val_loss: 3.8685 - val_sparse_categorical_accuracy: 0.6672
Epoch 63/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8618 - sparse_categorical_accuracy: 0.9136  
Epoch 00063: saving model to training_1/cp_0063.ckpt
47250/47250 [==============================] - 228s 5ms/sample - loss: 1.8618 - sparse_categorical_accuracy: 0.9137 - val_loss: 1.9150 - val_sparse_categorical_accuracy: 0.8878
Epoch 64/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8576 - sparse_categorical_accuracy: 0.9138  
Epoch 00064: saving model to training_1/cp_0064.ckpt
47250/47250 [==============================] - 228s 5ms/sample - loss: 1.8576 - sparse_categorical_accuracy: 0.9138 - val_loss: 2.1046 - val_sparse_categorical_accuracy: 0.8571
Epoch 65/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8588 - sparse_categorical_accuracy: 0.9132  
Epoch 00065: saving model to training_1/cp_0065.ckpt
47250/47250 [==============================] - 226s 5ms/sample - loss: 1.8589 - sparse_categorical_accuracy: 0.9131 - val_loss: 3.1942 - val_sparse_categorical_accuracy: 0.7149
Epoch 66/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8540 - sparse_categorical_accuracy: 0.9169  
Epoch 00066: saving model to training_1/cp_0066.ckpt
47250/47250 [==============================] - 226s 5ms/sample - loss: 1.8540 - sparse_categorical_accuracy: 0.9168 - val_loss: 1.8390 - val_sparse_categorical_accuracy: 0.9185
Epoch 67/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8524 - sparse_categorical_accuracy: 0.9154  
Epoch 00067: saving model to training_1/cp_0067.ckpt
47250/47250 [==============================] - 227s 5ms/sample - loss: 1.8523 - sparse_categorical_accuracy: 0.9154 - val_loss: 3.2358 - val_sparse_categorical_accuracy: 0.7611
Epoch 68/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8523 - sparse_categorical_accuracy: 0.9154  
Epoch 00068: saving model to training_1/cp_0068.ckpt
47250/47250 [==============================] - 227s 5ms/sample - loss: 1.8523 - sparse_categorical_accuracy: 0.9154 - val_loss: 1.9493 - val_sparse_categorical_accuracy: 0.8842
Epoch 69/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8464 - sparse_categorical_accuracy: 0.9185  
Epoch 00069: saving model to training_1/cp_0069.ckpt
47250/47250 [==============================] - 226s 5ms/sample - loss: 1.8464 - sparse_categorical_accuracy: 0.9185 - val_loss: 1.8216 - val_sparse_categorical_accuracy: 0.9248
Epoch 70/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8499 - sparse_categorical_accuracy: 0.9162  
Epoch 00070: saving model to training_1/cp_0070.ckpt
47250/47250 [==============================] - 226s 5ms/sample - loss: 1.8499 - sparse_categorical_accuracy: 0.9162 - val_loss: 2.4364 - val_sparse_categorical_accuracy: 0.7792
Epoch 71/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8427 - sparse_categorical_accuracy: 0.9200  
Epoch 00071: saving model to training_1/cp_0071.ckpt
47250/47250 [==============================] - 240s 5ms/sample - loss: 1.8427 - sparse_categorical_accuracy: 0.9200 - val_loss: 1.8469 - val_sparse_categorical_accuracy: 0.9137
Epoch 72/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8472 - sparse_categorical_accuracy: 0.9177  
Epoch 00072: saving model to training_1/cp_0072.ckpt
47250/47250 [==============================] - 263s 6ms/sample - loss: 1.8473 - sparse_categorical_accuracy: 0.9177 - val_loss: 2.2354 - val_sparse_categorical_accuracy: 0.8253
Epoch 73/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8427 - sparse_categorical_accuracy: 0.9188  
Epoch 00073: saving model to training_1/cp_0073.ckpt
47250/47250 [==============================] - 260s 6ms/sample - loss: 1.8427 - sparse_categorical_accuracy: 0.9188 - val_loss: 2.1518 - val_sparse_categorical_accuracy: 0.8246
Epoch 74/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8413 - sparse_categorical_accuracy: 0.9191  
Epoch 00074: saving model to training_1/cp_0074.ckpt
47250/47250 [==============================] - 262s 6ms/sample - loss: 1.8413 - sparse_categorical_accuracy: 0.9191 - val_loss: 2.4608 - val_sparse_categorical_accuracy: 0.8116
Epoch 75/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8369 - sparse_categorical_accuracy: 0.9217  
Epoch 00075: saving model to training_1/cp_0075.ckpt
47250/47250 [==============================] - 263s 6ms/sample - loss: 1.8369 - sparse_categorical_accuracy: 0.9218 - val_loss: 2.6312 - val_sparse_categorical_accuracy: 0.7888
Epoch 76/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8417 - sparse_categorical_accuracy: 0.9184  
Epoch 00076: saving model to training_1/cp_0076.ckpt
47250/47250 [==============================] - 251s 5ms/sample - loss: 1.8418 - sparse_categorical_accuracy: 0.9184 - val_loss: 2.0293 - val_sparse_categorical_accuracy: 0.8653
Epoch 77/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8360 - sparse_categorical_accuracy: 0.9219  
Epoch 00077: saving model to training_1/cp_0077.ckpt
47250/47250 [==============================] - 231s 5ms/sample - loss: 1.8360 - sparse_categorical_accuracy: 0.9219 - val_loss: 1.8322 - val_sparse_categorical_accuracy: 0.9187
Epoch 78/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8388 - sparse_categorical_accuracy: 0.9204  
Epoch 00078: saving model to training_1/cp_0078.ckpt
47250/47250 [==============================] - 229s 5ms/sample - loss: 1.8388 - sparse_categorical_accuracy: 0.9204 - val_loss: 1.9433 - val_sparse_categorical_accuracy: 0.8722
Epoch 79/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8321 - sparse_categorical_accuracy: 0.9215  
Epoch 00079: saving model to training_1/cp_0079.ckpt
47250/47250 [==============================] - 249s 5ms/sample - loss: 1.8322 - sparse_categorical_accuracy: 0.9215 - val_loss: 4.1766 - val_sparse_categorical_accuracy: 0.6611
Epoch 80/80
47232/47250 [============================>.] - ETA: 0s - loss: 1.8346 - sparse_categorical_accuracy: 0.9220  
Epoch 00080: saving model to training_1/cp_0080.ckpt
47250/47250 [==============================] - 261s 6ms/sample - loss: 1.8346 - sparse_categorical_accuracy: 0.9220 - val_loss: 2.5545 - val_sparse_categorical_accuracy: 0.7859

Test Data shape: (22500, 224, 224, 3)
Latest model : training_1/cp_0080.ckpt
22500/1 - 42s - loss: 2.1239 - sparse_categorical_accuracy: 0.7829
Accuracy: 78.29%
Y_train label distrib: (array([0, 1, 2, 3, 4]), array([10485, 10576, 10450, 10456, 10533]))
Y_test label distrib: (array([0, 1, 2, 3, 4]), array([4515, 4424, 4550, 4544, 4467]))
Y_pred_label distrib: (array([0, 1, 2, 3, 4]), array([3344, 3781, 4279, 5113, 5983]))
Number of different predicted labels: (2, 5)
Accuracy: 0.7829333333333334
Confusion matrix: 
[[3295   11  168  191  850]
 [  34 3051  214  212  913]
 [   8   97 3560  360  525]
 [   0  106  161 4146  131]
 [   7  516  176  204 3564]]
Classification Report:
               precision    recall  f1-score   support

           0       0.99      0.73      0.84      4515
           1       0.81      0.69      0.74      4424
           2       0.83      0.78      0.81      4550
           3       0.81      0.91      0.86      4544
           4       0.60      0.80      0.68      4467

    accuracy                           0.78     22500
   macro avg       0.81      0.78      0.79     22500
weighted avg       0.81      0.78      0.79     22500